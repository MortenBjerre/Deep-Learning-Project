{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starpilot-PPO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_WKdcrI6w3"
      },
      "source": [
        "# PPO and ProcGen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmf6lolwSqrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d40f022-9aa4-4969-e521-174006108363"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdpZ4lmFHtD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "208df5ea-93bd-467d-ca51-51f1c775d3f9"
      },
      "source": [
        "!pip install procgen"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: procgen in /usr/local/lib/python3.6/dist-packages (0.10.4)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (3.0.12)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (1.19.4)\n",
            "Requirement already satisfied: gym3<1.0.0,>=0.3.3 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.3.3)\n",
            "Requirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.17.3)\n",
            "Requirement already satisfied: imageio-ffmpeg<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (0.3.0)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (2.9.0)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.14.4)\n",
            "Requirement already satisfied: moderngl<6.0.0,>=5.5.4 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (5.6.2)\n",
            "Requirement already satisfied: glfw<2.0.0,>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.3.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (7.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.20)\n",
            "Requirement already satisfied: glcontext<3,>=2 in /usr/local/lib/python3.6/dist-packages (from moderngl<6.0.0,>=5.5.4->gym3<1.0.0,>=0.3.3->procgen) (2.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<1.0.0,>=0.15.0->procgen) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn2rkllGJPtZ"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z8P1ehENCwc"
      },
      "source": [
        "# Hyperparameters\n",
        "total_steps = 8e6\n",
        "num_envs = 32\n",
        "num_levels = 1000\n",
        "num_steps = 256 \n",
        "num_epochs = 3\n",
        "batch_size = 1024\n",
        "eps = .2\n",
        "grad_eps = .5\n",
        "value_coef = .5\n",
        "entropy_coef = .01\n",
        "learning_rate = 4e-4\n",
        "optimizer_eps = 1e-7"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt_e6hpzVm6s"
      },
      "source": [
        "import os\n",
        "os.chdir(r'/content/drive/My Drive/Colab Notebooks/Deep learning/Project')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils import make_env, Storage, orthogonal_init\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd \n",
        "import time"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOJ_sui6GeIr"
      },
      "source": [
        "def GPU_setup():\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "  else:\n",
        "    device = torch.device('cpu')\n",
        "GPU_setup() "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgkS-hN3c1lm"
      },
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvrRMdTW7Z_n"
      },
      "source": [
        "from torch import exp\n",
        "\n",
        "class PPO_loss():\n",
        "  def __call__(self, new_log_pi, old_log_pi, advantage, epsilon: float):\n",
        "    # compute ppo loss\n",
        "    ratio = exp(new_log_pi - old_log_pi)\n",
        "    clipped_ratio = ratio.clamp(min = 1 - epsilon, max = 1 + epsilon)\n",
        "    reward = torch.min(ratio * advantage, clipped_ratio * advantage)\n",
        "\n",
        "    return - reward.mean()\n",
        "\n",
        "class clipped_value_loss():\n",
        "  def __call__(self, new_value, old_value, old_return, epsilon: float):\n",
        "    # compute clipped value loss\n",
        "    clipped_value = old_value + (new_value - old_value).clamp(min = -epsilon, max = epsilon)\n",
        "    value_function_loss = torch.max((new_value - old_return) ** 2, (clipped_value - old_return) ** 2)\n",
        "    return 0.5 * (value_function_loss.mean())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCpWLhY_EG28"
      },
      "source": [
        "def xavier_uniform_init(module, gain=1.0):\r\n",
        "    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\r\n",
        "        nn.init.xavier_uniform_(module.weight.data, gain)\r\n",
        "        nn.init.constant_(module.bias.data, 0)\r\n",
        "    return module"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0zu34_Fqdda"
      },
      "source": [
        "import math\r\n",
        "def f(x):\r\n",
        "    '''\r\n",
        "    Used to calculate the number of output features from each IMPALA block.\r\n",
        "    '''\r\n",
        "    x = math.sqrt(x)\r\n",
        "    x = math.ceil(x/2)\r\n",
        "    return x**2"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTBV9xpKpEFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9902e8b-fb5f-426a-daad-3f5bddff7294"
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class NatureDQNEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, feature_dim):\n",
        "        super().__init__()\n",
        "        #input : 64 x 64 x 3 (pixel x pixel x rgb)\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), \n",
        "            nn.ReLU(), # 15 x 15 x 32\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), \n",
        "            nn.ReLU(), # 6 x 6 x 64\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), \n",
        "            nn.ReLU(), # 4 x 4 x 64 = 1024\n",
        "            Flatten(),\n",
        "            nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n",
        "        )\n",
        "        self.apply(orthogonal_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class ImpalaEncoder_2(nn.Module):\n",
        "    '''\n",
        "    Used to test the expanded version of the impala encoder where\n",
        "    the channels have been scaled by a factor of 2 compared to the standard. \n",
        "    '''\n",
        "    def __init__(self, in_channels, feature_dim):\n",
        "        super().__init__()\n",
        "        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=32)\n",
        "        self.block2 = ImpalaBlock(in_channels=32, out_channels=64)\n",
        "        self.block3 = ImpalaBlock(in_channels=64, out_channels=64)\n",
        "        self.fc = nn.Linear(in_features = 64 * 8 * 8, out_features = feature_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.apply(xavier_uniform_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.relu(x)\n",
        "        x = Flatten()(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class ImpalaEncoder_3(nn.Module):\n",
        "    '''\n",
        "    Used to test the expanded version of the impala encoder where\n",
        "    the channels have been scaled by a factor of 2 compared to the standard\n",
        "    and an additional fully connected layer was added. \n",
        "    '''\n",
        "    def __init__(self, in_channels, feature_dim):\n",
        "        super().__init__()\n",
        "        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=32)\n",
        "        self.block2 = ImpalaBlock(in_channels=32, out_channels=64)\n",
        "        self.block3 = ImpalaBlock(in_channels=64, out_channels=64)\n",
        "        self.fc1 = nn.Linear(in_features = 64 * 8 * 8, out_features = 2048)\n",
        "        self.fc2 = nn.Linear(in_features = 2048, out_features = feature_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.apply(xavier_uniform_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.relu(x)\n",
        "        x = Flatten()(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    '''\n",
        "    Used in the IMPALA blocks.\n",
        "    '''\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__() # <-\n",
        "        self.convolution1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.convolution2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.convolution1(self.relu(x))\n",
        "        output = self.convolution2(self.relu(output))\n",
        "        return output + x\n",
        "\n",
        "class ImpalaBlock(nn.Module):\n",
        "    '''\n",
        "    Used in the IMPALA encoder. The IMPALA encoder will use three of these\n",
        "    blocks with varying in- and out-channels.\n",
        "    '''\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.convolution = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.residual_block1 = ResidualBlock(out_channels)\n",
        "        self.residual_block2 = ResidualBlock(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convolution(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.residual_block1(x)\n",
        "        x = self.residual_block2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ImpalaEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, feature_dim):\n",
        "        super().__init__()\n",
        "        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16)\n",
        "        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n",
        "        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n",
        "        self.fc = nn.Linear(in_features=32 * 8 * 8, out_features = feature_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.apply(xavier_uniform_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.relu(x)\n",
        "        x = Flatten()(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    '''\n",
        "    Policy class used to map from screen input to action distribution and\n",
        "    value function. \n",
        "    '''\n",
        "    def __init__(self, encoder, feature_dim, num_actions): \n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
        "        self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
        "        self.ppo_loss = PPO_loss()\n",
        "        self.value_loss = clipped_value_loss()\n",
        "\n",
        "    def act(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = x.cuda().contiguous()\n",
        "            dist, value = self.forward(x)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "\n",
        "        return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        logits = self.policy(x)\n",
        "        value = self.value(x).squeeze(1)\n",
        "        dist = Categorical(logits=logits)\n",
        "\n",
        "        return dist, value\n",
        "\n",
        "# Define environment\n",
        "env = make_env(num_envs, num_levels = num_levels)\n",
        "# num_levels defines the number of levels the AI can train on\n",
        "print('Observation space:', env.observation_space)\n",
        "print('Action space:', env.action_space.n)\n",
        "\n",
        "# Define network\n",
        "#encoder = Encoder(3, 1024)\n",
        "encoder = NatureDQNEncoder(3, 1024)\n",
        "policy = Policy(encoder, 1024, env.action_space.n)\n",
        "policy.cuda()\n",
        "\n",
        "# Define optimizer\n",
        "# these are reasonable values but probably not optimal\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate, eps=optimizer_eps)\n",
        "\n",
        "# Define temporary storage\n",
        "# we use this to collect transitions during each iteration\n",
        "storage = Storage(\n",
        "    env.observation_space.shape,\n",
        "    num_steps,\n",
        "    num_envs,\n",
        "    gamma = 1.0\n",
        ")\n",
        "\n",
        "\n",
        "create_policy_from_checkpoint = False\n",
        "# Load from previous checkpoint\n",
        "if 'checkpoint.pt' in os.listdir() and create_policy_from_checkpoint:\n",
        "    print('Checkpoint in folder.')\n",
        "    checkpoint = torch.load(os.getcwd() + '/checkpoint.pt')\n",
        "    policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    print('Policy loaded from checkpoint!')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
            "Action space: 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqHlYIgEc5IU"
      },
      "source": [
        "# Evaluation method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jZUoJvHZCbX"
      },
      "source": [
        "def evaluate_policy(policy, num_envs=32):\r\n",
        "    '''\r\n",
        "    Evaluation function. Let's `num_envs` play until they are all death.\r\n",
        "    Once an environments dies it's reward will no longer be counted.\r\n",
        "    The function returns the average reward across the `num_envs` and the \r\n",
        "    std as well. \r\n",
        "    '''\r\n",
        "    # Make evaluation environment\r\n",
        "    eval_env = make_env(num_envs, start_level = num_levels, num_levels = num_levels) \r\n",
        "    obs = eval_env.reset()\r\n",
        "    total_reward = []\r\n",
        "    # Evaluate policy\r\n",
        "    policy.eval()\r\n",
        "\r\n",
        "    live_envs = np.array([True] * num_envs)\r\n",
        "    while sum(live_envs) != 0: # runs until all environments are done\r\n",
        "        # Use policy\r\n",
        "        action, log_prob, value = policy.act(obs)\r\n",
        "\r\n",
        "        # Take step in environment\r\n",
        "        obs, reward, done, info = eval_env.step(action)\r\n",
        "        \r\n",
        "        reward = [x['reward'] for x in info] #unnormalized reward\r\n",
        "        reward = reward * live_envs # don't count reward if environment is done\r\n",
        "        # update done environments\r\n",
        "        level_done = np.array( [x['prev_level_complete'] for x in info] )\r\n",
        "        # done is also true if the level is complete, so in that case we do not\r\n",
        "        # want the ai to stop playing but continue with the next level\r\n",
        "        live_envs = (np.invert(done) | level_done)  * live_envs \r\n",
        "        total_reward.append(torch.Tensor(reward))\r\n",
        "\r\n",
        "        t += 1\r\n",
        "    # Calculate average return\r\n",
        "    total_reward = torch.stack(total_reward).sum(0)\r\n",
        "    average_reward = total_reward.mean(0)\r\n",
        "    average_reward_std = total_reward.std()\r\n",
        "\r\n",
        "    return average_reward.item(), average_reward_std.item()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yt32L68c7EM"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04r4nsGDzTc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f688dd2-dbdf-4f1f-ec17-19682b691f8d"
      },
      "source": [
        "# Run training\n",
        "obs = env.reset()\n",
        "step = 0\n",
        "std, step_list, mean_reward = [], [], []\n",
        "penalty_for_dying = 10\n",
        "step_list_eval = []\n",
        "evaluation_score = []\n",
        "evaluation_score_std = []\n",
        "c = 0\n",
        "start_training_time = time.time()\n",
        "\n",
        "\n",
        "while step < total_steps:\n",
        "    # Use policy to collect data for num_steps steps\n",
        "    policy.eval()\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        # Use policy\n",
        "        action, log_prob, value = policy.act(obs)\n",
        "        \n",
        "        # Take step in environment\n",
        "        next_obs, reward, done, info = env.step(action)\n",
        "        #reward = reward - done * penalty_for_dying - used in an experiment\n",
        "                \n",
        "        # Store data\n",
        "        storage.store(obs, action, reward, done, info, log_prob, value)\n",
        "        \n",
        "        # Update current observation\n",
        "        obs = next_obs\n",
        "\n",
        "    # Add the last observation to collected data\n",
        "    _, _, value = policy.act(obs)\n",
        "    storage.store_last(obs, value)\n",
        "\n",
        "    # Compute return and advantage\n",
        "    storage.compute_return_advantage()\n",
        "    \n",
        "    # Optimize policy\n",
        "    policy.train()\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Iterate over batches of transitions\n",
        "        generator = storage.get_generator(batch_size)\n",
        "        for batch in generator:\n",
        "            b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
        "\n",
        "            # Get current policy outputs\n",
        "            new_dist, new_value = policy(b_obs)\n",
        "            new_log_prob = new_dist.log_prob(b_action)\n",
        "\n",
        "            # Clipped policy objective\n",
        "            pi_loss = policy.ppo_loss(new_log_prob, b_log_prob, b_advantage, eps)\n",
        "\n",
        "            # Clipped value function objective\n",
        "            value_loss = policy.value_loss(new_value, b_value, b_returns, eps)\n",
        "\n",
        "            # Entropy loss\n",
        "            entropy_loss = new_dist.entropy().mean()\n",
        "\n",
        "            # Backpropagate losses\n",
        "            loss =  pi_loss + value_coef * value_loss - entropy_coef * entropy_loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients\n",
        "            torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
        "\n",
        "            # Update policy\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    #evaluation during training \n",
        "    if c % 100 == 0: # done appr. every 800k step\n",
        "        e_score, e_std = evaluate_policy(policy)\n",
        "        evaluation_score.append(e_score)\n",
        "        evaluation_score_std.append(e_std)\n",
        "        step_list_eval.append(step)\n",
        "    c += 1\n",
        "\n",
        "    # Update stats\n",
        "    step += num_envs * num_steps\n",
        "\n",
        "    step_list.append(step)\n",
        "    mean_reward.append(storage.get_reward().item())\n",
        "    std.append(storage.reward.sum(0).std().item())\n",
        "    print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n",
        "\n",
        "\n",
        "print('Completed training!')\n",
        "end_training_time = time.time()\n",
        "#save policy\n",
        "torch.save({\n",
        "    'policy_state_dict':policy.state_dict(), \n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "    },'checkpoint.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 8192\tMean reward: 3.71875\n",
            "Completed training!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAZrWuVGLTu-"
      },
      "source": [
        "## Saving the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee79VvBcbaJQ"
      },
      "source": [
        "We mainly used the HPC on DTU to train the AI but it was saved in the following way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awz8iZ-VRqN-"
      },
      "source": [
        "folder_name = time.ctime()\r\n",
        "folder_path = r'/content/drive/My Drive/Colab Notebooks/Deep learning/Project/Data/'\r\n",
        "os.mkdir((folder_path + folder_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvDYn_Pah71R"
      },
      "source": [
        "Our updated evaluation of the policy runs until all of the 32 environments are killed by a spaceship (with the limitation of 20,000 steps so that the script doesn't potentially gets stuck if AI won't die - it's about 13 minutes and it never came close). Once an environment dies it's reward is no longer counted. The end of a level is also signaled by `done = True`, but with the exception that `prev_level_complete = 1` in `info`. In this case we do not register the environment as dead and let it play the next level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zecOCkd7Jzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "389fc67c-087b-42f6-f4d6-4bd2c7e2f84a"
      },
      "source": [
        "import imageio\n",
        "\n",
        "# Make evaluation environment\n",
        "eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels) # 1 i stedet for num_envs \n",
        "obs = eval_env.reset()\n",
        "\n",
        "frames = []\n",
        "total_reward = []\n",
        "# Evaluate policy\n",
        "policy.eval()\n",
        "\n",
        "live_envs = np.array([True] * num_envs)\n",
        "t = 0\n",
        "while sum(live_envs) != 0 or t > 20000: # runs until all environments are done or 20k steps\n",
        "    # Use policy\n",
        "    action, log_prob, value = policy.act(obs)\n",
        "\n",
        "    # Take step in environment\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    \n",
        "    reward = [x['reward'] for x in info] #unnormalized reward\n",
        "    reward = reward * live_envs # don't count reward if environment is done\n",
        "    # update done environments\n",
        "    level_done = np.array( [x['prev_level_complete'] for x in info] )\n",
        "\n",
        "    # done is also true if the level is complete, so in that case we do not\n",
        "    # want the ai to stop playing but continue with the next level\n",
        "    live_envs = (np.invert(done) | level_done)  * live_envs \n",
        "\n",
        "    total_reward.append(torch.Tensor(reward))\n",
        "\n",
        "    # Render environment and store\n",
        "    frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n",
        "    frames.append(frame)\n",
        "\n",
        "    t += 1\n",
        "\n",
        "# Calculate average return\n",
        "total_reward = torch.stack(total_reward).sum(0)\n",
        "average_reward = total_reward.mean(0)\n",
        "print('Average return:', average_reward)\n",
        "imageio.mimsave('vid.mp4', frames, fps=25) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average return: tensor(21.6400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVrvaHdpOwNQ"
      },
      "source": [
        "# Save frames as video\n",
        "frames = torch.stack(frames)\n",
        "imageio.mimsave((folder_path + folder_name)+'/vid.mp4', frames, fps=25) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCDm6kfjQw6s"
      },
      "source": [
        "We save the mean reward across all environments and the standard deviation of the mean reward, along with the steps so we can easily plot it (in data_visuals.ipynb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cPxXrdNN6K_"
      },
      "source": [
        "# Save training in csv\n",
        "\n",
        "comment = \"\"\"\n",
        "tester lige hvordan vi evaluerer\n",
        "\"\"\"\n",
        "d = {'total_steps'          : total_steps,\n",
        "      'num_envs'            : num_envs,\n",
        "      'num_levels'          : num_levels,\n",
        "      'num_steps'           : num_steps,\n",
        "      'num_epochs'          : num_epochs,\n",
        "      'batch_size'          : batch_size,\n",
        "      'eps'                 : eps,\n",
        "      'grad_eps'            : grad_eps,\n",
        "      'value_coef'          : value_coef,\n",
        "      'entropy_coef'        : entropy_coef,\n",
        "      'learning_rate'       : learning_rate,\n",
        "      'optimizer_eps'       : optimizer_eps,\n",
        "      'eval_average_reward' : average_reward.item(),\n",
        "      'eval_std'            : total_reward.std().item(),\n",
        "      'training_time_min'   : (end_training_time - start_training_time)/60,\n",
        "      'encoder'             : 'NatureDQN',\n",
        "      'comments'            : comment}\n",
        "hyperparams = pd.DataFrame([d])\n",
        "\n",
        "hyperparams.to_csv((folder_path + folder_name) + '/hyperparameters.csv', index = False)\n",
        "\n",
        "d = {'step': step_list,\n",
        "     'std_reward': std,\n",
        "     'mean_reward': mean_reward}\n",
        "data = pd.DataFrame(d)\n",
        "data.to_csv((folder_path + folder_name) + '/data.csv', index = False)\n",
        "\n",
        "\n",
        "d = {'step': step_list_eval,\n",
        "     'std_reward': evaluation_score_std,\n",
        "     'mean_reward': evaluation_score}\n",
        "\n",
        "data_evaluation = pd.DataFrame(d)\n",
        "data_evaluation.to_csv((folder_path + folder_name) + '/eval_training.csv', index = False)\n",
        "\n",
        "torch.save({\n",
        "    'policy_state_dict':policy.state_dict(), \n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, (folder_path + folder_name) + '/checkpoint.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFLNrNuzS38i"
      },
      "source": [
        "## Re evaluate training results \r\n",
        "\r\n",
        "We updated the evaluation function and therefore need to reevaluate our results, which we can since we saved the policies from the HPC results. \r\n",
        "\r\n",
        "We did this for all the results in the data folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj06P_Mecmrc"
      },
      "source": [
        "path = '/content/drive/MyDrive/Project/Data/random'\r\n",
        "os.chdir(path)\r\n",
        "for i, folder in enumerate(os.listdir(path)):\r\n",
        "  #print(folder)\r\n",
        "  # Define network\r\n",
        "  #encoder = ImpalaEncoder(3, 1024)\r\n",
        "\r\n",
        "  encoder = ImpalaEncoder(3, 1024)\r\n",
        "  policy = Policy(encoder, 1024, env.action_space.n)\r\n",
        "  policy.cuda()\r\n",
        "\r\n",
        "  # Define optimizer\r\n",
        "  # these are reasonable values but probably not optimal\r\n",
        "  optimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate, eps=1e-5)\r\n",
        "\r\n",
        "  # Define temporary storage\r\n",
        "  # we use this to collect transitions during each iteration\r\n",
        "  storage = Storage(\r\n",
        "      env.observation_space.shape,\r\n",
        "      num_steps,\r\n",
        "      num_envs,\r\n",
        "      gamma = 1.0\r\n",
        "  )\r\n",
        "\r\n",
        "  # Load from previous checkpoint\r\n",
        "  checkpoint = torch.load(folder + '/checkpoint.pt')\r\n",
        "  policy.load_state_dict(checkpoint['policy_state_dict'])\r\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
        "  #policy.eval()\r\n",
        "  print('Policy loaded from checkpoint!')\r\n",
        "\r\n",
        "\r\n",
        "  # Make evaluation environment\r\n",
        "  hyperparams = pd.read_csv(folder + '/hyperparams.csv')\r\n",
        "\r\n",
        "  eval_env = make_env(num_envs, start_level=hyperparams.num_levels.item(), \r\n",
        "                      num_levels=hyperparams.num_levels.item()) \r\n",
        "  obs = eval_env.reset()\r\n",
        "\r\n",
        "  #frames = []\r\n",
        "  total_reward = []\r\n",
        "  # Evaluate policy\r\n",
        "  policy.eval()\r\n",
        "\r\n",
        "  live_envs = np.array([True] * num_envs)\r\n",
        "\r\n",
        "  while sum(live_envs) != 0: # runs until all environments are done\r\n",
        "    # Use policy\r\n",
        "    action, log_prob, value = policy.act(obs)\r\n",
        "\r\n",
        "    # Take step in environment\r\n",
        "    obs, reward, done, info = eval_env.step(action)\r\n",
        "    reward = [x['reward'] for x in info] #unnormalized reward\r\n",
        "    reward = reward * live_envs # don't count reward if environment is done\r\n",
        "    # update done environments\r\n",
        "    level_done = np.array([x['prev_level_complete'] for x in info])\r\n",
        "\r\n",
        "    # done is also true if the level is complete, so in that case we do not\r\n",
        "    # want the ai to stop playing but continue with the next level\r\n",
        "    live_envs = (np.invert(done) | level_done)  * live_envs \r\n",
        "\r\n",
        "    total_reward.append(torch.Tensor(reward))\r\n",
        "\r\n",
        "    # Render environment and store\r\n",
        "    #frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\r\n",
        "    #frames.append(frame)\r\n",
        "\r\n",
        "  # Calculate average return\r\n",
        "  total_reward = torch.stack(total_reward).sum(0)\r\n",
        "  average_reward = total_reward.mean(0)\r\n",
        "  print('Average return:', average_reward.item())\r\n",
        "  \r\n",
        "  hyperparams['eval_average_reward'] = average_reward.item()\r\n",
        "  hyperparams['eval_std'] =  total_reward.std().item()\r\n",
        "  hyperparams.to_csv(folder + '/hyperparams.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0cTAseYhLG8"
      },
      "source": [
        "# Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Ajx4JRbsgL"
      },
      "source": [
        "Block for making the video of the AI playing the game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWRT1igKURN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514c6abf-e1dd-4028-b820-d869fa19391d"
      },
      "source": [
        "# Define environment\r\n",
        "# check the utils.py file for info on arguments\r\n",
        "env = make_env(num_envs, num_levels = num_levels)\r\n",
        "print('Observation space:', env.observation_space)\r\n",
        "print('Action space:', env.action_space.n)\r\n",
        "\r\n",
        "# Define network\r\n",
        "#encoder = Encoder(3, 1024)\r\n",
        "encoder = ImpalaEncoder(3, 1024)\r\n",
        "policy = Policy(encoder, 1024, env.action_space.n)\r\n",
        "policy.cuda()\r\n",
        "\r\n",
        "# Define optimizer\r\n",
        "# these are reasonable values but probably not optimal\r\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate, eps=1e-5)\r\n",
        "\r\n",
        "# Define temporary storage\r\n",
        "# we use this to collect transitions during each iteration\r\n",
        "storage = Storage(\r\n",
        "    env.observation_space.shape,\r\n",
        "    num_steps,\r\n",
        "    num_envs,\r\n",
        "    gamma = 1.0\r\n",
        ")\r\n",
        "\r\n",
        "create_policy_from_checkpoint = True\r\n",
        "os.chdir(r'/content/drive/MyDrive/Project/Data/impala_eps/Mon Dec 28 03%3A51%3A33 2020')\r\n",
        "# Load from previous checkpoint\r\n",
        "if 'checkpoint.pt' in os.listdir() and create_policy_from_checkpoint:\r\n",
        "    print('Checkpoint in folder.')\r\n",
        "    checkpoint = torch.load(os.getcwd() + '/checkpoint.pt')\r\n",
        "    policy.load_state_dict(checkpoint['policy_state_dict'])\r\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
        "    #policy.eval()\r\n",
        "    print('Policy loaded from checkpoint!')\r\n",
        "\r\n",
        "import imageio\r\n",
        "from random import randint\r\n",
        "\r\n",
        "# Make evaluation environment\r\n",
        "eval_env = make_env(1, start_level=1000, num_levels=1000, seed = randint(1,1000)) \r\n",
        "obs = eval_env.reset()\r\n",
        "frames = []\r\n",
        "policy.eval()\r\n",
        "\r\n",
        "t=0\r\n",
        "while t < 1500: # play for a fixed number of steps\r\n",
        "    # Use policy\r\n",
        "    action, log_prob, value = policy.act(obs)\r\n",
        "\r\n",
        "    # Take step in environment\r\n",
        "    obs, reward, done, info = eval_env.step(action)\r\n",
        "    # Render environment and store\r\n",
        "    frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\r\n",
        "    frames.append(frame)\r\n",
        "    t += 1\r\n",
        "\r\n",
        "os.chdir(r'/content/drive/MyDrive/Project')\r\n",
        "imageio.mimsave('best_policy.mp4', frames, fps=25) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
            "Action space: 15\n",
            "Checkpoint in folder.\n",
            "Policy loaded from checkpoint!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a4c0VkxJ9m5"
      },
      "source": [
        "# New evaluation method test\r\n",
        "\r\n",
        "The following code was used to test the evaluation function where each environment had 3 (or more) lives each instead of just 1. It did not help reduce the std. of the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmsNTZ9LKFs9",
        "outputId": "d97a1dde-2f8f-48ec-d4c3-361a7c34fcbf"
      },
      "source": [
        "# Define environment\r\n",
        "# check the utils.py file for info on arguments\r\n",
        "eval_env = make_env(32, start_level=num_levels, num_levels=num_levels) \r\n",
        "print('Observation space:', env.observation_space)\r\n",
        "print('Action space:', env.action_space.n)\r\n",
        "\r\n",
        "# Define network\r\n",
        "#encoder = Encoder(3, 1024)\r\n",
        "encoder = ImpalaEncoder(3, 1024)\r\n",
        "policy = Policy(encoder, 1024, env.action_space.n)\r\n",
        "policy.cuda()\r\n",
        "\r\n",
        "# Define optimizer\r\n",
        "# these are reasonable values but probably not optimal\r\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate, eps=1e-5)\r\n",
        "\r\n",
        "# Define temporary storage\r\n",
        "# we use this to collect transitions during each iteration\r\n",
        "storage = Storage(\r\n",
        "    env.observation_space.shape,\r\n",
        "    num_steps,\r\n",
        "    num_envs,\r\n",
        "    gamma = 1.0\r\n",
        ")\r\n",
        "\r\n",
        "create_policy_from_checkpoint = True\r\n",
        "os.chdir(r'/content/drive/MyDrive/Project/Data/data_impala/Fri Dec 11 08%3A52%3A13 2020')\r\n",
        "# Load from previous checkpoint\r\n",
        "if 'checkpoint.pt' in os.listdir() and create_policy_from_checkpoint:\r\n",
        "    print('Checkpoint in folder.')\r\n",
        "    checkpoint = torch.load(os.getcwd() + '/checkpoint.pt')\r\n",
        "    policy.load_state_dict(checkpoint['policy_state_dict'])\r\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
        "    #policy.eval()\r\n",
        "    print('Policy loaded from checkpoint!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
            "Action space: 15\n",
            "Checkpoint in folder.\n",
            "Policy loaded from checkpoint!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljL_fl6Fi7be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d412a3b-834f-4a88-affd-0427854ae99f"
      },
      "source": [
        "def evaluate_policy(policy, num_levels, num_envs=32, lives_per_environment = 5):\r\n",
        "    results = []\r\n",
        "    for i in range(lives_per_environment):\r\n",
        "        print(i)\r\n",
        "        total_reward = []\r\n",
        "        eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels) \r\n",
        "        obs = eval_env.reset()\r\n",
        "        live_envs = np.array([True] * num_envs)\r\n",
        "        while sum(live_envs) != 0: # runs until all environments are done \r\n",
        "            # Use policy\r\n",
        "            action, log_prob, value = policy.act(obs)\r\n",
        "\r\n",
        "            # Take step in environment\r\n",
        "            obs, reward, done, info = eval_env.step(action)\r\n",
        "            \r\n",
        "            reward = [x['reward'] for x in info] #unnormalized reward\r\n",
        "            reward = reward * live_envs # don't count reward if environment is done\r\n",
        "            # update done environments\r\n",
        "            level_done = np.array( [x['prev_level_complete'] for x in info] )\r\n",
        "\r\n",
        "            # done is also true if the level is complete, so in that case we do not\r\n",
        "            # want the ai to stop playing but continue with the next level\r\n",
        "            live_envs = (np.invert(done) | level_done)  * live_envs \r\n",
        "\r\n",
        "            total_reward.append(torch.Tensor(reward))\r\n",
        "\r\n",
        "            # Render environment and store\r\n",
        "\r\n",
        "        # Calculate average return\r\n",
        "        total_reward = torch.stack(total_reward).sum(0)\r\n",
        "        results.append(total_reward.tolist())\r\n",
        "    return np.array(results) #, average_reward.item(), average_reward_std.item()\r\n",
        "\r\n",
        "r = evaluate_policy(policy, 1000, num_envs = 32, lives_per_environment = 5)\r\n",
        "k = r.mean(axis=0)\r\n",
        "print(k.mean())\r\n",
        "print(k.std())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "28.96875\n",
            "30.265992027975887\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}